# ðŸ“Š Customer Churn Prediction Pipeline (Databricks + Medallion Architecture)

![Databricks](https://img.shields.io/badge/Platform-Databricks-red) ![Spark](https://img.shields.io/badge/Engine-PySpark-orange) ![ML](https://img.shields.io/badge/ML-LogisticRegression%20%7C%20XGBoost%20%7C%20CatBoost-blue) ![License](https://img.shields.io/badge/License-MIT-green)

A scalable **end-to-end machine learning pipeline** built on **Databricks** following the **Medallion Architecture** (Bronze âž” Silver âž” Gold).
The pipeline performs **data ingestion with Autoloader**, feature engineering using **PySpark**, and **ML model training** using multiple classification algorithms (Logistic Regression, Random Forest, XGBoost, CatBoost).

---

## ðŸš€ Project Overview

This repository demonstrates a **production-ready churn prediction workflow**:

* **Bronze Layer** â€“ Load raw data from Delta tables with Databricks Autoloader.
* **Silver Layer** â€“ Clean, normalize, and encode categorical features with PySpark.
* **Gold Layer** â€“ Prepare ML-ready feature set and train ML models using Scikit-learn, XGBoost, and CatBoost.

The final output is a **trained classification model** for predicting customer churn.

---

## ðŸ—ï¸ Architecture (Medallion)

```mermaid
flowchart LR
    A[Bronze Layer\nRaw Delta Data] --> B[Silver Layer\nCleaned & Encoded Data]
    B --> C[Gold Layer\nFeature Store + ML-Ready Dataset]
    C --> D[Model Training\nLogistic Regression, XGBoost, CatBoost]
    D --> E[Model Evaluation\nAccuracy, Precision, Recall, F1]
```

* **Bronze:** Raw ingestion using `spark.readStream.format("cloudFiles")` or batch `delta` reads.
* **Silver:** Data quality improvements, feature engineering, missing value handling, one-hot encoding.
* **Gold:** Bucketization, type casting, final schema for ML models.

---

## ðŸ§© Key Features

âœ… **Databricks Autoloader** â€“ Automatically detects and loads new data into Bronze.
âœ… **PySpark Transformations** â€“ Feature engineering at scale (One-Hot Encoding, Bucketization).
âœ… **Medallion Architecture** â€“ Clean, layered design for reproducibility and governance.
âœ… **ML Training** â€“ Logistic Regression, Random Forest, XGBoost, CatBoost, Neural Networks.
âœ… **Metrics Tracking** â€“ Accuracy, Precision, Recall, F1-score, Confusion Matrix, ROC-AUC.
âœ… **Scalable & Modular** â€“ Can be deployed to production with minimal changes.

---

## ðŸ“‚ Repository Structure

```bash
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€dev
â”‚     â”œâ”€â”€ nb_raw    # Autoloader setup & raw data ingestion
â”‚     â”œâ”€â”€ nb_bronze    # Data cleaning, feature encoding
â”‚     â””â”€â”€ nb_silver  # Final dataset for ML
â”‚   â”œâ”€â”€uat
â”‚   â”œâ”€â”€prod
â”œâ”€â”€ .gitignore                 
â”œâ”€â”€ README.md                         
â””â”€â”€ LICENSE
```

---

## ðŸ”‘ Example Workflow

1. **Ingest Data**

```python
DATA_PATH = "/Volumes/workspace/bronze/bronzevolume/data/"
df = spark.read.format("delta").load(DATA_PATH)
```

2. **Feature Engineering**

* One-hot encode categorical columns
* Rename boolean columns (`Yes/No â†’ 1/0`)
* Bucketize continuous variables (e.g., `MonthlyCharges`, `TotalCharges`)
* Cast all features to proper numeric types

3. **Train/Test Split**

```python
from sklearn.model_selection import train_test_split

X = pdf.drop("churn", axis=1)
y = pdf["churn"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

4. **Model Training**

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=2500)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

5. **Evaluation**

```python
from sklearn.metrics import accuracy_score, f1_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print(f"Accuracy: {accuracy}, F1 Score: {f1}")
print(classification_report(y_test, y_pred))
```

---

## âš¡ Dependencies

* Python 3.9+
* Databricks Runtime (with PySpark)
* pandas, numpy, matplotlib, seaborn, plotly, missingno
* scikit-learn, xgboost, catboost

---

## ðŸ“œ License

## This project is licensed under the MIT License â€“ see the [LICENSE](LICENSE) file for details.
